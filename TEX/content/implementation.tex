% !TeX root = ../SDonchezResearchIReport.tex

\section{EDF Algorithm Implementation}\label{sec:Impl}

This effort implemented the above algorithm in a test application which is representative of its implementation in the real-world use case outlined in Section \ref{subsec:Proposal}. However, as the scope of the research completed to date does not include the larger system outlined in that section, it is somewhat constrained with regards to its external interfaces. Namely, it does not attempt to directly affect any actual IP Cores, but rather performs theoretical scheduling of the cores and notes its results in a log file. It does, however, attempt to mimic the likely real-world method of input, in that it parses serialized data such as would be suitable for transmission by a discrete centralized controler overseeing a number of PFPGA instances.

Along with the development of the Scheduler itself, it was also necessary to throughly exercise the application to ensure that it functions sucessfully under load. As a result, it was also necessary to develop a testbench application, which is capable of generating arbitrary tasks for the AES cores to schedule. This testbench is capable of generating tasks in accordance with a given target utilization rate. A comprehensive set of interprocess communication (IPC) calls enable synchronization between the two applications. For purposes of ensuring low resource utilization, as well as promoting well organized source code, both of these applications were developed in C++, utilizing an object-oriented approach. Section \ref{subsec:SchedulerImpl}, below, outlines the implementation of the scheduler itself, while Section \ref{subsec:TestbenchImpl} outlines the implementation of the testbench. Section \ref{subsec:LinuxImpl} outlines the environment within which the applications run, while \ref{subsec:IPC} outlines the interconnection of these two applications.

\subsection{Scheduler Implementation}\label{subsec:SchedulerImpl}
The scheduler application consists of several interrelated threads operating in parallel. First among these is an input parser, which waits for serialized task information and, upon receipt, deserializes it into task objects. For the sake of best practices (as well as ease of parsing), this data is encapsulated in a JavaScript Object Notation (JSON) string, which, despite its name, is a language agnostic structue that is commonly used to represent entities. In its current implementation this data is simply read from a file descriptor (such as standard input), although future extensibility to a socket based architecture for network communications is possible.

Alongside the input parser is the timer manager. The timer manager is responsible for managing the discrete units of time for which the cores can be scheduled. The AES cores intended for use in the larger research effort are currently non-pipelined cores requiring a fixed number of clock cycles to process each word of data. Accorddingly, the frequency with which they can be scheduled is based on a multiple of that numebr of clock cycles, which must be small enough to afford flexibility but high enough to minimize wasted cost due to context switching (per the design assumptions). As the HPS and the PL do not necessarily share a common clock, and as the execution of the various threads on the processor is non-deterministic in its own scheduling, it is necessary to utilize a timer to ensure scheduling operations are executed exactly once per atomic unit of IP Core schedulability. This thread provides that functionality, and sets flags for the other threads as needed to facilitate their operation.

The final (and arguably most consequential) thread of the scheduler application is the core servicer itself, which is responsible for ensuring each core is executing the ideal task per the prescribed algorithm. Once per Time Unit, the core servicer evaluates the deadlines of the tasks currently executing on the cores against those contained in the scheduler's Task Queue, and performs substitutions as necessary to ensure that the tasks with the nearest deadlines are those currently in execution. 

For purposes of this effort, these services are implemented using C++'s standard threading library (as opposed to the more traditional "pthread" library). This threading implementation was chosen to promote cross-platform interoperability, as well as performance optimization where possible.


\subsection{Testbench Implementation}\label{subsec:TestbenchImpl}
The Testbench application in in many ways appreciably simpler than the Scheduler itself. It is only a single threaded application, and the task it performs is straightforward - it simply creates tasks populated with arbitrary information. However, this seemingly straightforward operation is greatly complicated by one of the assumptions outlined in Section \ref{subsec:EDFAssumptions} above. Namely it is the fourth assumption in that section which proves problematic: The CSP will not allocate more decryption jobs to the PFPGA than can be scheduled within their given deadlines. This greatly eased the development of the Scheduler itself, by eliminating the need for it to perform schedulability checks as part of its task processing. However, as the Testbench is effectively serving as the CSP in these simulations, it falls to the testbench to ensure that the tasks it generates do not overload the PFPGA instance for which the Scheduler is allocating resources.

This is accomplished through introducing some element of reporting to the Scheduler itself, and making those results available to the testbench. Specifically, the scheduler maintains a table outlining how many outstanding time units need to be executed by any given point in time, as described by the sum of all of the outstanding units for each task due at that point. By combining this information with knowledge of what unit in time the PFPGA and PS are currently in, it is possible to validate in real time the schedulability of a task as it is being generated, and to make adjustments as needed. 

This logic is captured in Equation \ref{equ:Utilization}, below. In this equation, $t_c$ represents the current Time Unit, $t_n$ represents the time unit being proposed as the deadline for the task, $D_i$ represents the number of units due at time $i$, and $UtE$ represents the number of Units to Execute for the task currently being evaluated. $P$ is the parallelization factor, or the number of cores which the schedule is being distributed between, while $n-c$ yields the total number of TimeUnits between the time the equation is evaluated and when the deadline will come. The "Desired Load Percentage" is the target utilization rate for the cores.

\begin{equation}\label{equ:Utilization}
    \frac{(\sum_{i=t_c}^{t_n}D_i)+UtE}{P*(n-c)} < Desired\:Load\:Percentage
\end{equation}

By verifying the validity of this equation for each generated task's proposed deadline, it is possible to ensure that schedulability constraints are not violated. Furthermore, the maintenance of this information on the part of the scheduler is non-resource intensive, requiring a single addition operation as each task is parsed, a single decrement operation as each core is serviced, and a single increment operation (for the current unit counter) on the part of timer manager per Time Unit.

\subsection{Operating System and Environment}\label{subsec:LinuxImpl}
By virtue of introducing multiple threads to the scheduler (and also by virtue of running two applications on a single core processor), it is necessary to implement an operating system (OS) on the processor system upon which the scheduler simulation is running. Futhermore, the larger design will add additional functionality to the processor system, further necessitating an operating system for thread management and for scheduling of the processor itself. To this end, several operating system choices were considered for implementation on the hardware target in support of this simulation. As the target in question includes a Xilnx SoC, it was decided that the ideal choice for such a system would be the Xilinx PetaLinux OS, as it includes all of the necessary drivers for interfacing with the various devices on the development platform. Additionally, the PetaLinux platform is based on the OpenEmbedded Project's toolchain, which affords tremendous flexibilty and customization potential as needed.

For ease of development and testing, the two software applications were designed to run both in the native Windows environment of the development computer as well as the Linux environment of the target. This reduced cycle time between incremental iterations of the applications, as well as eliminated dependence on an extensive physical setup for testing.

\subsection{InterProcess Communications}\label{subsec:IPC}
The inclusion of an operating system into this research effort greatly eased efforts to facilitate communication between the two discrete applications. Both Windows and Linux implement comprehensive facilities for InterProcess Communication (IPC), which enable the transfer of data between applications. Unfortunately, the two systems do not have a common set of IPC functions, requiring slightly difference implementations to satisfy the two architecures. Despite this, the overal design of the IPC structures for the two architectures is effectively the same.

\subsubsection{Testbench to Scheduler Communication}\label{subsubsec:TestbenchSchedIPC}
Communication from the Testbench application to the Scheduler application is straightforward, as the only traffic flowing in this direction is the serialized task data. Since this data is serialized and relatively small in size, and in the interest of replicating the real-world functionality of the software, this data is simply read in from a file descriptor. In Windows, this is accomplished via a Named Pipe, which effectively enables the data to be consumed as if it were typed into the standard input of the application in the terminal window in which it is executing. Specifically, this named pipe was configured in "message" mode, such that the entire task is treated as an atomic unit for purposes of the transfer, rather than its constituent bytes (the latter confirguration can result in multiple or even partial messages being sent as a single input, which would unnecessarily complicate the parsing operation).

Linux also provides a "Named Pipe" functionality, but this functionality is strictly byte-oriented as opposed to message-based. Accordingly, it is not desirable for use in this scenario. Fortunately, the Linux system also supports System-V Message Queues, a part of the Posix standard that is functionaly equivalent to Windows's Named Pipes in message mode.

\subsubsection{Scheduler to Testbench Communication}\label{subsubsec:SchedTestbenchIPC}
The transfer of data from the Scheduler to the Testbench is appreciaby more complex, as the table of Outstanding Units Due (used in Equation \ref{equ:Utilization}) is updated during each TimeUnit, and contains $P*N$ words of data, where N is the number of time units to simulate and P is the number of IP Cores. It would be inefficient to the point of impracticability to transfer and parse that data every time unit, as its scale for even modest simulations would likely consume a non-trivial portion of the available compute time to parse. Accordingly, the implementation instead uses a shared memory space for the purposes of facilitating this communication, accomplished in Windows by utilizing the File Mapping functionality of the Win32 API, and in Linux via the comparable Shared Memory Segment functionality. In this way, the generator simply receives a pointer to the scheduler's own copy of the table, and is always able to view the latest updates to it without any need for direct transfer between the applications.

The same functionality is utilized for the mapping of the current Time Unit between the two systems, as it eliminates any edge cases wherein the unit changes shortly after the testbench queries it. This allows for the most accurate possible scheduling of tasks.